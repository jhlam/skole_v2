\chapter{Related Work}  \label{relatedWork}


In this chapter, we will look at the state of research regarding High-Level Synthesis, information diffusion, and optimization of independent cascade model and breadth-first search.

\section{Information Diffusion} 
There are multiple studies done regarding information diffusion. Studies show how information diffusion can be applied during an disease outbreak\cite{InformationDiffusionThroughBlogspace}, viral marketing\cite{ViralMarketing}, coordinat during crisis situation\cite{Starbird:2012:RRI:2145204.2145212}. 

Different models of information diffusion have been done on blogs\citep{Adar:2005:TIE:1092358.1092473}\cite{GomezRodriguez:2010:IND:1835804.1835933}, and Twitter\cite{Bakshy:2011:EIQ:1935826.1935845}. We can see that in an age of social media, the studies of information diffusion is more relevant than ever. 

While \cite{InfoDiffAndExternalInfluInNetworks} have argued that the emerging of social networks and media have changed the traditional model. The activation is no longer only relying on neighbour vertices, but also an external influence. They found that a large amount of information volume in Twitter is the result of network diffusion, while a small amount is due to external events and factors outside the network\citep{InfoDiffAndExternalInfluInNetworks}. Another study shows that during the 2011 Egyptian Uprising, how a large amount of rebel movement were "tweeted"\cite{Starbird:2012:RRI:2145204.2145212} during the uprising.

As we mentioned in Chapter \ref{background}, we mainly focus on two common information diffusion models, ICM, and LTM. But there are different models too. \cite{5694014} proposed several different problems with traditional models where each vertex is either \textit{activated}(infected, influenced, '1') or \textit{inactive}(healthy, not reached, '0'), and passes the \textit{contagion}(information, data, infection, influence) to neighbouring vertices through the edges. The report mentioned different assumptions that such models make. Among them is that a complete graph is provided, the spread of contagion is from a known source, and that the structure of the network is sufficient to explain the behaviour\citep{5694014}. The report proposes an alternative model, \textit{Linear Influence Model}(LIM), where the focus is on the global influence that an infected vertex has on the rate of diffusion through the implicit network. This model makes the assumption that newly activated vertices are dependent on previously activated vertices. The LIM does not need explicit knowledge of the entire network. Instead, the model takes the newly activated vertices and models them as a \textit{influence function}, which is used to find the global influence.     

\section{High-Level Synthesis}
High-Level Synthesis as a concept has been around since the mid-1980s and early-1990s\cite{HLSTutorial}\citep{HLSPastFutur}. Carnegie-Mellon University design automation (CMU-DA)\citep{1085036}\citep{Parker:1979:CDA:800292.811694} was a pioneering early version of HLS tools. The tool gathers quickly considerable interest. Many HLS tools were built in later years mostly for prototyping and research\cite{Granacki:1985:AAD:317825.317970}\citep{Paulin:1986:HMA:318013.318055}\citep{4069894}. Some of these were able to produce real chips, but the reason for the lack of further development and adaptation was that RTL synthesis was not a widely accepted and an immature field. This often lead to suboptimal solutions. 

Around the year 2000, new HLS tools were developed in academia and the industry. These tools, used high-level language, C and C++. Vivado HLS, designed by Xilinx \citep{6409453}, is one such HLS tool. The Vivado HLS became free during their 2015.4 update\citep{VIVADOHLS}. This resulted in a revived interest in HLS. The community around HLS is also evolving, on the Xilinx-forum, there are multiple answers and active members. We can see that the solution designed by HLS tools is close to traditional hand-crafted designs\citep{6718388}.

Cong et al,\cite{HLSTutorial} discusses different problems and reason for failed commercialization of early HLS tools.  concluded that the problem with the early HLS tools can be summarized by the following reasons: "Lack of comprehensive design language support", "Lack of reusable and portable design specification", "Narrow focus on datapath synthesis", "lack satisfactory QoR", and "Lack of a compelling Reason/event to adopt new design methodology".

 Early versions of HLS tools was not a C-to-RTL transformation. Most of them needed a custom \textit{Hardware Descriptive Language}(HDL). Lacking reusable and portable design specification resulted in that HLS tools required users to include detailed information regarding timing and interface information into source codes. This resulted in a target dependent solution and can't easily port to other devices. The narrow focus on datapath synthesis resulted in a lack of focus on an interface to other hardware modules and platform integration. Those aspects were left to the users to solve system integration problem. The lack of foundation to accurately measure HLS result and often failed to meet timing and power requirement with early HLS tools were another limiting factor. The last reason was that there was no real driving force to turn developers over to such a young and early development format. HLS tools showed exciting capabilities, but most developers did not want to move from the safe and tested RTL design methodology. The paper concludes with that current(2011)HLS tools showed tremendous potentials in becoming standard in selected deployment.

\subsection{Applications using HLS}
In \citep{Malazgirt:2015:HLS:2889287.2889299}, HLS was used to design an accelerator for database analytic and SQL operation. The design was implemented on a Virtex-7 xc7vx690t-g1761-2 FPGA with a focus on accelerating operations; join, data filter, sort, merge and string matching. The accelerator was implemented in C++ in Vivado HLS and optimized with UNROLL directive, PIPELINE directive, and ARRAY\_PARTITION. The UNROLL directive unroll all of the specified loops, while the PIPELINE directive allows multiple accelerators to process data at each clock cycle. The ARRAY\_PARTITION directive partition data into registers.  The accelerator showed promises, giving a 15-140$\times$ speedup compared to Postgres software DBMS running selected TPC\-H queries. 

\cite{Bailey:2015:ALH:2789116.2789145} explored the advantages and disadvantages of HLS implementation of image processing. They argued that custom algorithms on FPGA platforms will most likely result in an improvement, but the algorithms must be tailored to the platforms. The author conducted different case studies to show both the strengths and the weaknesses of HLS. The report goes through image filtering, connected component analysis and two-dimensional fast-Fourier transformation(FFT). One example that the author brings up is during image filtering where HLS was not able to identify the standard accessing pattern during special cases. This resulted in that the HLS built additional hardware to counter such an exception. The report concluded that while HLS can significantly reduce development time and improve utilization of the design space, it is still important to focus on careful design.  The report concludes that HLS can offer many benefits, and is an improvement over conventional RTL-designs, but is not a replacement for hardware designers or clever designs. 

\cite{Butt:2016:DPH:2888407.2871737} implemented a fast Fourier transform (FFT) algorithm for different digital system processing application in HLS. There the authors used Simulink for verification of their design, and implemented it in HLS.

\cite{Zuo:2013:IHL:2435264.2435271} discuss improvements to the current HLS tools with polyhedral transformation. Here they present a problem with HLS, which is that unless the code is inherently compatible, HLS can not apply most of the optimizations. Zuo et al. proposed the polyhedral model, the model takes data dependent multi-block program as input and performs three steps: Classification of array access patterns, performance Metric, and implementation. During the classification of array access patterns, a set of data access pattern is defined and classified. Then the appropriate loop transformation is applied. The next step, the performance of each loop transformation with data-dependency is estimated, and the best improvement is chosen. In the final step, the chosen solution, loop transformation, and inserting HLS directive are applied. Then an interface block for the data-dependent blocks is generated. The generated communication block is then optimized depending on how it behaves. The paper concludes with that the polyhedral model can model can find important loop transformation, thus enable optimization such as pipeline and parallelization.

\cite{6718388} is a case-study where HLS is used to implement two compute heavy machine learning techniques with different computational properties. The two algorithms that were tested was \textit{Lloyd's Algorithm} and a \textit{Filtering Algorithm}. The result was that for the first case, a similar performance between the HLS solution and a hand-written solution, while the second algorithm was severely worse with HLS if the developer did not customize for HLS.
 
\section{Different optimization scheme}
There are a large amount of different optimization research on graph traversal, especially on Breadth First Search. 

\citep{HybridBFS2015} proposed a hybrid FPGA-CPU heterogeneous platform for BFS. The idea is to run the first couple of steps on the CPU core, then switch over to the FPGA accelerator to explore the rest of the graph. The CPU is better suited for calculating while there is a smaller frontier, while the FPGA core is better suited for a larger frontier. By exploiting the characteristics of small-world networks where the frontier is much larger after two-three iterations, one can significantly improve computation time. The report proposed an alternative method of performing the breadth-first search. By performing it as a sparse matrix-vector multiplication over a boolean semiring, more parallelization option was discovered. The result was a speedup of 7.8 compared to a pure software implementation ,and 2$\times$ better compared to an accelerator-only implementation. 
 
\citep{beamer2013direction} propose a hybrid solution, combining a conventional top-down algorithm and a novel bottom-up algorithm. The optimization in this paper focuses on examining fewer edges, thus reduce computation time and trying to circumvent one major drawback with BFS; memory-bound on shared memory. The top-down approach is the traditional algorithm, where a frontier expands and visits all vertices on that level, before each vertex checks its neighbour for unvisited vertices. Unvisited vertices are placed in the frontier vector and marked as visited. The bottom-up algorithm, contrary to the top -down the algorithm, is where each children vertex tries to find a potential parent. A neighbour vertex can be the parent if the neighbour is also in the frontier vector. This results in that after a vertex finds its parent, there is no need to traverse the rest of the frontier. By using different approach during different time, the report was able to achieve a speedup of 3.3 - 7.8$\times$ on synthetic graphs and 2.4 â€“ 4.6$\times$ on real social network graphs.