\chapter{Related Work}  \label{relatedWork}


In this chapter, we will look at the state of research regarding High Level Synthesis, information diffusion, and optimization of independent cascade model and  breadth-first search.

\section{Information Diffusion} 
There are multiple studies done regarding information diffusion. One study shows how information diffusion can be applied during an disease outbreak\cite{InformationDiffusionThroughBlogspace}, viral marketing\cite{ViralMarketing}, coordinat during crisis situation\cite{Starbird:2012:RRI:2145204.2145212}. 

Different models of information diffusion have been done on blogs\citep{Adar:2005:TIE:1092358.1092473}\cite{GomezRodriguez:2010:IND:1835804.1835933}, and twitter\cite{Bakshy:2011:EIQ:1935826.1935845}. We can see that in an age of social media, the studies of information diffusion is more relevant then ever. 

While \cite{InfoDiffAndExternalInfluInNetworks} have argued that the emerging of social networks and media, have changed the traditional model. The activation is no longer only relying on neighbour nodes, but also an external influence. They found that large amount of information volume in Twitter is the result of network diffusion, while a small amount is due to external events and factors outside the network\citep{InfoDiffAndExternalInfluInNetworks}. Another studie shows that during the 2011 Egyptian Uprising, how a large amount of rebel movement were "tweeted"\cite{Starbird:2012:RRI:2145204.2145212} during the uprising.

As we mentioned in Chapter \ref{background}, we mainly focus on two common information diffusion models, ICM and LTM. But there are different models too. \cite{5694014} proposed several different problems with traditional models where each node is either \textit{activated}(infected, influenced, '1') or \textit{inactive}(healthy, not reached, '0'), and passes the \textit{contagion}(information, data, infection, influence) to neighbouring nodes through the edges. The report mentioned different assumptions that such models make. Among them is that a complete graph is provided, the spread of contagion is from a known source, and that the structure in the network is sufficient to explain the the behaviour\citep{5694014}. The report propose an alternative model, \textit{Linear Influence Model}(LIM), where the focus is on the global influence that an infected node has on the rate of diffusion through the implicit network. This model makes the  assumption that newly activated nodes are dependent on previous activated nodes. The LIM does not need explicit knowledge of the entire network, instead the model takes the newly activated nodes and model them as a \textit{influence function}, which is used to find the global influence. 	

\section{High Level Synthesis}
High Level Synthesis as a concept have been around since the mid-1980s and early-1990s\cite{HLSTutorial}\citep{HLSPastFutur}. Carnegie-Mellon University design automation (CMU-DA)\citep{1085036}\citep{Parker:1979:CDA:800292.811694} was a pioneering early version of HLS tools. The tool gathers quickly considerable interest. A number of HLS tools were built in later years mostly for prototyping and research\cite{Granacki:1985:AAD:317825.317970}\citep{Paulin:1986:HMA:318013.318055}\citep{4069894}. Some of these was able to produce real chips, but the reason for lack of further development and adaptation, was that RTL synthesis was not a widely accepted and a immature field. This often lead to suboptimal solutions. 

Around the year 2000, new HLS tools was developed in academia and in the industry. These tools, used hihg level language, C and C++. Vivado HLS, designed by Xilinx \citep{6409453}, is one such HLS tool. The Vivado HLS became free during their 2015.4 update\citep{VIVADOHLS}. This resulted in an revived interest in HLS. The community around HLS is also evolving, on the Xilinx-forum, there are multiple answers and active members. We can see that the solution designed by HLS tools is close to traditional hand-crafted designs\citep{6718388}.

\cite{HLSTutorial} goes into the history of HLS throughout the ages.The paper discuss different problems and reason for failed commercialization of early HLS tools, Cong et al, concluded that the problem with the early HLS tools can be summarized by the following reasons: \textit{Lack of comprehensive design language support}, \textit{Lack of reusable and portable design specification}, \textit{Narrow focus on datapath synthesis}, \textit{lack satisfactory QoR}, \textit{Lack of a compelling Reason/event to adopt new design methodology}.
%Set i HERMETEGN%
 Early version of HLS tools was not a C-to-RTL transformation. Most of them needed a custom \textit{Hardware Descriptive Language}(HDL). Lacking of reusable and portable design specification resulted in that HLS tools required user to include detailed information regarding timing and interface information into source code. This resulted in a target dependent solution and can't easily port to other devices. The narrow focus on datapath synthesis resulted in a lack of focus on interface to other hardware modules and platform integration. Those aspect was left to the users to solve system integration problem. The lack of foundation to accurately measure HLS result and often failed ot meet timing and power requirement with early HLS tools were another limiting factor. The last reason, was that there where no real driving force to turn developers over to such a young and early development format. HLS tools shows interesting capabilities, but most developers did not want to move from the safe and tested RTL design methodology. The paper concludes with that current(2011)HLS tools showed huge potentials in becoming standard in selected deployment.

\subsection{Applications using HLS}
In \citep{Malazgirt:2015:HLS:2889287.2889299}, HLS was used to design an accelerator for database analytic and SQL operation. The design was implemented on a Virtex-7 xc7vx690t-g1761-2 FPGA with focus on accelerating operations as join, data filter, sort, merge and string matching. The accelerator was implemented in C++ in Vivado HLS and optimized with UNROLL directive, PIPELINE directive and ARRAY\_PARTITION. The UNROLL directive unroll all of the specified loops, while the PIPELINE directive allows multiple accelerator to process data at each clock cycle. The ARRAY\_PARTITION directive partition data into registers.  The accelerator showed promises, giving a 15-140$\times$ speedup compared to Postgres software DBMS running selected TPC\-H queries. 

\cite{Bailey:2015:ALH:2789116.2789145} explored the advanteage and disadvanteges of HLS implementation of image processing. They argued that custom algorithms on FPGA platforms will most likely result in an improvement, but the algorithms must be tailored to the platforms. The author conducted different case studies to show both the strengths and the weaknesses of HLS. The report goes through image filtering, connected component analysis and two dimensional fast-Fourier transformation(FFT). One example that the author brings up, is during image filtering where HLS was not able to identify the standard accessing pattern during special cases. This resulted in that the HLS built additional hardware to counter such a exception. The report concluded that while HLS can significantly reduce development time and improve utilization of the design space, it is still important to focus on careful design.  The report concludes that HLS can offer many benefits, and is an improvement over conventional RTL-designs, but is not an replacement for hardware designers or clever designs. 

\cite{Butt:2016:DPH:2888407.2871737} implemented an fast Fourier transform (FFT) algorithm for different digital system processing application in HLS. There the authors used Simulink for verification of design, and implemented it in HLS.

\cite{Zuo:2013:IHL:2435264.2435271} discuss improvements to the current HLS tools with  polyhedral transformation. Here they discuss a problem with HLS, which is that unless the code is inherently compatible, HLS can not apply most of the optimizations. Zuo et al. proposed the polyhedral model, the model takes data dependent multi-block program as input and performs three steps: Classification of array access patterns, performance Metric, and implementation. During the classification of array access patterns, a set of data access pattern is defined and classified. Then the appropriate loop transformation is applied. The next step, the performance of each loop transformation with data-dependency is estimated, and the best improvement is chosen. In the final step, the chosen solution, loop transformation and inserting HLS directive is applied. Then a interface block for the data-dependent blocks is generated. The generated communication block is then optimized depending on how it behaves. The paper concludes with that the polyhedral model can model can find important loop transformation, thus enable optimization such as pipeline and parallelization.

\cite{6718388} is a case-study where HLS is used to implement two compute heavy machine learning techniques with different computational properties. The two algorithms that was tested was \textit{Lloyd's Algorithm} and a \textit{Filtering Algorithm}. The result was that for the first case, a similar performance between the HLS solution and a hand-written solution, while the second algorithm was severely worse with HLS, if the developer did not customized for HLS.
 
\section{Different optimization scheme}
There are a large amount of different optimization research on graph traversal, especially on Breadth First Search. 

\citep{HybridBFS2015} proposed a hybrid FPGA-CPU heterogeneous platform for BFS. The idea is to run the first couple of steps on the CPU core, then switch over to the FPGA accelerator to explore the rest of the graph. The CPU is better suited for calculating while there is a smaller frontier, while the FPGA core is better suited for a larger frontier. By exploiting the characteristics of small-world networks where the frontier is much larger after two-three iterations, one  can significantly improve computation time. The report proposed an alternative method of performing breadth-first search. By performing it as a sparse matrix vector multiplication over a boolean semiring, more parallilization option was discovered. The result was a speedup of 7.8 compared to a pure software implementation ,and 2$\times$ better compared to an accelerator only implementation. 
 
\citep{beamer2013direction} propose a hybrid solution, combining a conventional top-down algorithm and a novel bottom-up algorithm. The optimization in this paper focuses on examining fewer edges, thus reduce computation time and trying to circumvent one major drawback with BFS; memory-bound on shared memory. The top down approach is the traditional algorithm, where a frontier expands and visits all nodes on that level, before each node checks its neighbour for unvisited vertices. Unvisited vertices is placed in the frontier and marked as visited. The bottom up algorithm, contrary to the top down algorithm, is where each children vertex tries to find a potential parent. A neighbour vertex can be parent if the neighbour is also in the frontier. This results in that after a vertex finds its parent, there is no need to traverse the rest of the frontier. By using different approach during different time, the report was able to achieve a speedup of 3.3 - 7.8$\times$ on synthetic graphs and 2.4 – 4.6$\times$ on real social network graphs.

\citep{Agarwal:2010:SGE:1884643.1884670}
